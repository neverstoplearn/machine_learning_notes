{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "wiki: https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal linear transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. \n",
    "\n",
    "This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "<img src=\"https://s2.ax1x.com/2019/10/06/ugWAC6.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "## Method\n",
    "Observation set $X$ contains $N$ samples and each sample $x_n \\in R^p$.   Transformation matrix $W$ contains $p$ column vectors, each vector $w_k \\in R^p$.\n",
    "Principle component score\n",
    "$$s_{nk} = x_n w_k$$\n",
    "\n",
    "In order to maximize variance, the first weight vector $w_1$ thus has to satisfy\n",
    "$$w_1 = \\arg\\max_{w_1} \\sum_n s_{nk}^2=\\arg\\max_{w_1} W^T X^T X W,$$\n",
    "$$s.t. ||W||=1$$\n",
    "\n",
    "Since $w_1$ has been defined to be a unit vector, it equivalently also satisfies\n",
    "$$w_1 = \\arg\\max_{w_1} \\frac{W^T X^T X W}{W^T W}$$\n",
    "\n",
    "A standard result for a positive semidefinite matrix such as $X^T X$ is that the maximum possible value is the largest eigenvalue of the matrix, which occurs when $w_1$ is the corresponding eigenvector.\n",
    "\n",
    "The full principal components decomposition of X can therefore be given as\n",
    "$$T = XW$$\n",
    "where W is a p-by-p matrix of weights whose columns are the eigenvectors of $X^T X$.\n",
    "\n",
    "Principal component analysis is also a technique for dimension reduction. It combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables.\n",
    "\n",
    "## Algorithm\n",
    "1. Take the matrix of independent variables $X$ and, for each column, subtract the mean of that column from each entry and obtain $Z$. (This ensures that each column has a mean of zero.)\n",
    "2. Take the matrix $Z$, transpose it, and multiply the transposed matrix by $Z$. The resulting matrix is the covariance matrix $Z^T Z$.\n",
    "3. Calculate the eigenvectors and their corresponding eigenvalues of $Z^{T}Z$. \n",
    "4. Take the eigenvalues $λ_1, λ_2, …, λ_p$ and sort them from largest to smallest. In doing so, sort the eigenvectors in $W$ accordingly to obtain $W^*$.\n",
    "5. Calculate $Z^* = ZW^*$. This new matrix, $Z^*$, is a centered/standardized version of $X$ but now each observation is a combination of the original variables, where the weights are determined by the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pca_image/1.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/2.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/3.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/4.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/5.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/6.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/7.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/8.png\" width=640 height=320 />\n",
    "<img src=\"./pca_image/9.png\" width=640 height=320 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法步骤\n",
    "1.pca预处理  对X每一列按行做0均值变换 得到均值为0的矩阵Z\n",
    "\n",
    "2.得到$Z^T$Z(协方差矩阵)\n",
    "\n",
    "3.计算协方差矩阵的特征值和特征向量,并将特征值从大到小排列 同理将特征向量按照特征值排列，得到变换矩阵$W^*$\n",
    "\n",
    "4.计算$Z^*=ZW^*$(取$W^*$前k列即压缩至K维)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_custom(X, k):\n",
    "    n, p = np.shape(X)\n",
    "    # make Z with mean zero\n",
    "    Z = X - np.mean(X, axis=0)\n",
    "    # calculate covariance maxtrix\n",
    "    covZ = np.cov(Z.T)#####np.cov接收数据是按列排列的数据 需要转置\n",
    "    # calculate eigenvalues and eigenvectors\n",
    "    eigValue, eigVector = np.linalg.eig(covZ)\n",
    "    # sort eigenvalues in descending order\n",
    "    index = np.argsort(-eigValue)###np.argsort得到的是从小打到排列的索引 因此添加负号\n",
    "    \n",
    "    # select k principle components\n",
    "    if k > p:\n",
    "        print (\"k must lower than input data dimension！\")\n",
    "        return 1\n",
    "    else:\n",
    "        # select k eigenvectors with largest eigenvalues\n",
    "        selectVector = eigVector[:, index[:k]]\n",
    "        T = np.matmul(Z, selectVector)\n",
    "    return T, selectVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_sklearn = True\n",
    "\n",
    "# generate data\n",
    "# x = np.random.randn(100, 20)\n",
    "\n",
    "y = np.random.randn(20, 1)\n",
    "x = np.matmul(y, [[1.3, -0.5]])\n",
    "\n",
    "# set k\n",
    "k = 2\n",
    "# PCA\n",
    "pcaX, selectVector = pca_custom(x, k)\n",
    "\n",
    "print('PCA transformation matrix: ')\n",
    "print(selectVector)\n",
    "\n",
    "if use_sklearn:\n",
    "    z = x - np.mean(x, axis=0)\n",
    "    pcaResults = PCA(n_components=k).fit(z)\n",
    "    print ('PCA transformation matrix from sklearn:')\n",
    "    print(pcaResults.components_.T)\n",
    "    newX = np.matmul(z, pcaResults.components_.T)\n",
    "\n",
    "print('original data')\n",
    "plt.plot(x[:, 0], x[:, 1], 'ok')\n",
    "plt.show()\n",
    "\n",
    "print('After PCA')\n",
    "plt.plot(pcaX[:, 0], pcaX[:, 1], 'or')\n",
    "plt.show()\n",
    "\n",
    "if use_sklearn:\n",
    "    print('After PCA from sklearn')\n",
    "    plt.plot(newX[:, 0], newX[:, 1], 'ob')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlayGround\n",
    "http://setosa.io/ev/principal-component-analysis/\n",
    "# Code\n",
    "https://github.com/csuldw/MachineLearning/tree/master/PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
