{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./ID3/1.png\" width=640 height=640 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625 0.375\n",
      "0.954434002925\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#### 计算信息熵示例\n",
    "init_dic = {\n",
    "    \"count\": [64,64,128,60,64,64,64,128,64,132,64,32,32,64],\n",
    "    \"age\": [\"青年\",\"青年\",\"中年\",\"老年\",\"老年\",\"老年\",\"中年\",\"青年\",\"青年\",\"老年\",\"青年\",\"中年\",\"中年\",\"老年\"],\n",
    "    \"income\": [\"高\",\"高\",\"高\",\"中\",\"低\",\"低\",\"低\",\"中\",\"低\",\"中\",\"中\",\"中\",\"高\",\"中\"],\n",
    "    \"student\": [\"否\",\"否\",\"否\",\"否\",\"是\",\"是\",\"是\",\"否\",\"是\",\"是\",\"是\",\"否\",\"是\",\"否\"],\n",
    "    \"reputation\": [\"良\",\"优\",\"良\",\"良\",\"良\",\"优\",\"优\",\"良\",\"良\",\"良\",\"优\",\"优\",\"良\",\"优\"],\n",
    "    \"purchase\": [\"不买\",\"不买\",\"买\",\"买\",\"买\",\"不买\",\"买\",\"不买\",\"买\",\"买\",\"买\",\"买\",\"买\",\"不买\"]\n",
    "}\n",
    "data = pd.DataFrame(init_dic, columns=[\"count\", \"age\", \"income\", \"student\", \"reputation\", \"purchase\"])\n",
    "\n",
    "# 计算买和不买的样本数据\n",
    "purchase_yes_count= data[data[\"purchase\"] == \"买\"][\"count\"].sum()\n",
    "purchase_no_count = data[data[\"purchase\"] == \"不买\"][\"count\"].sum()\n",
    "# 计算各自的概率\n",
    "purchase_yes_p = purchase_yes_count / (purchase_yes_count + purchase_no_count)\n",
    "purchase_no_p = 1 - purchase_yes_p\n",
    "print(purchase_yes_p, purchase_no_p)\n",
    "# 计算此时的信息熵\n",
    "I_purchase = -purchase_yes_p*np.log2(purchase_yes_p) -purchase_no_p*np.log2(purchase_no_p)\n",
    "print(I_purchase)\n",
    "\n",
    "# 0.625 0.375\n",
    "# 0.954434002924965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./ID3/2.png\" width=640 height=640 />\n",
    "<img src=\"./ID3/3.png\" width=640 height=640 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265712127384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25118\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log2\n",
      "C:\\Users\\25118\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "####计算age字段的(经验)条件熵以及它的信息增益\n",
    "\n",
    "def shannon(data, column=\"age\"):\n",
    "    # 找到这个字段的唯一值\n",
    "    levels = data[column].drop_duplicates().tolist()  # ['青年', '中年', '老年']\n",
    "    # 计算该字段的所有数据集，显然是整个数据集\n",
    "    samples = data[\"count\"].sum()\n",
    "    # 依次计算信息熵\n",
    "    entropy = 0\n",
    "    for level in levels:\n",
    "        # 获取该水平的子数据集，计算买与不买的信息熵\n",
    "        subdata = data[data[column] == level]\n",
    "        purchase_yes = subdata[subdata[\"purchase\"] == \"买\"][\"count\"].sum()\n",
    "        purchase_no = subdata[subdata[\"purchase\"] == \"不买\"][\"count\"].sum()\n",
    "        purchase_yes_p = purchase_yes / (purchase_yes + purchase_no)\n",
    "        purchase_no_p = 1 - purchase_yes_p\n",
    "        # 计算该水平上的信息熵\n",
    "        if purchase_yes == 0 or purchase_no == 0: # 这里要处理子数据集为空的情况；这里暂未处理\n",
    "            pass\n",
    "        I_purchase = -purchase_yes_p*np.log2(purchase_yes_p) -purchase_no_p*np.log2(purchase_no_p)            \n",
    "        # 计算该水平上的概率值\n",
    "        level_p = subdata[\"count\"].sum() / samples\n",
    "        # 计算信息增益\n",
    "        if I_purchase > 0:\n",
    "            entropy += level_p * I_purchase\n",
    "        # print(level, level_p, I_purchase, purchase_yes, purchase_no, entropy)\n",
    "    return entropy\n",
    "\n",
    "entropy_age = shannon(data, \"age\")\n",
    "gain_age = I_purchase - entropy_age  # 计算这个字段的信息增益\n",
    "print(gain_age)\n",
    "# 0.2657121273840979\n",
    "# 有报错0除，没做处理。本例只演示如何计算叶节点信息熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ID3/4.png\" width=640 height=640 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class LoadDataSet(object):\n",
    "    def load_dataSet(self):\n",
    "        data = pd.read_csv(\"./ID3/ID3data.txt\", sep=\"\\t\", header=None)\n",
    "        data.rename(columns={0: \"age\", 1: \"income\", 2: \"student\", 3: \"reputation\", 4: \"purchase\"}, inplace=True)\n",
    "        return data\n",
    "    \n",
    "class TreeHandler(object):\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "    def save(self, tree):\n",
    "        self.tree = tree\n",
    "        with open(\"./ID3/tree.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "            tree = json.dumps(tree, indent=\"  \", ensure_ascii=False)\n",
    "            f.write(tree)\n",
    "    def load(self, file):\n",
    "        with open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            tree = f.read()\n",
    "            self.tree = json.loads(tree)\n",
    "        return self.tree    \n",
    "\n",
    "class ID3Tree(LoadDataSet, TreeHandler):\n",
    "    \"\"\"主要的数据结构是pandas对象\"\"\"\n",
    "    __count = 0\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"认定最后一列是标签列\"\"\"\n",
    "        self.dataSet = self.load_dataSet()\n",
    "        self.gain = {}\n",
    "    \n",
    "    def _entropy(self, dataSet):\n",
    "        \"\"\"计算给定数据集的熵\"\"\"\n",
    "        labels= list(dataSet.columns)\n",
    "        #print(labels)\n",
    "        #print(dataSet[labels[-1]].value_counts())\n",
    "        level_count = dataSet[labels[-1]].value_counts().to_dict()  # 统计分类标签不同水平的值\n",
    "        #print(level_count)\n",
    "        entropy = 0.0\n",
    "        for key, value in level_count.items():\n",
    "            prob = float(value) / dataSet.shape[0]\n",
    "            entropy += -prob * np.log2(prob)\n",
    "        return entropy\n",
    "    \n",
    "    def _split_dataSet(self, dataSet, column, level):\n",
    "        \"\"\"根据给定的column和其level来获取子数据集\"\"\"\n",
    "        subdata = dataSet[dataSet[column] == level]\n",
    "        del subdata[column] # 删除这个划分字段列\n",
    "        return subdata.reset_index(drop=True)  # 重建索引\n",
    "    \n",
    "    def _best_split(self, dataSet):\n",
    "        \"\"\"计算每个分类标签的信息增益\"\"\"\n",
    "        best_info_gain = 0.0  # 求最大信息增益\n",
    "        best_label = None     # 求最大信息增益对应的标签(字段)\n",
    "        labels = list(dataSet.columns)[: -1]     # 不包括最后一个靶标签\n",
    "        #print(labels)\n",
    "        init_entropy = self._entropy(dataSet)  # 先求靶标签的香农熵即最后一列的熵\n",
    "        for _, label in enumerate(labels):\n",
    "            # 根据该label(也即column字段)的唯一值(levels)来切割成不同子数据集，并求它们的香农熵\n",
    "            levels = dataSet[label].unique().tolist()  # 获取该分类标签的不同level  即取a时  a有0 1两个值\n",
    "            #print(levels)\n",
    "            label_entropy = 0.0  # 用于累加各水平的信息熵；分类标签的信息熵等于该分类标签的各水平信息熵与其概率积的和。\n",
    "            for level in levels: # 循环计算不同水平的信息熵  ###当取a时 且a = 0  取出a = 0 的样例数 并计算其概率\n",
    "                level_data = dataSet[dataSet[label] == level]  # 获取该水平的数据集\n",
    "                prob = level_data.shape[0] / dataSet.shape[0]  # 计算该水平的数据集在总数据集的占比\n",
    "                # 计算香农熵，并更新到label_entropy中\n",
    "                label_entropy += prob * self._entropy(level_data) # _entropy用于计算香农熵  ###当取a时 计算熵\n",
    "            # 计算信息增益\n",
    "            info_gain = init_entropy - label_entropy  # 代码至此，已经能够循环计算每个分类标签的信息增益 ##\n",
    "            # 用best_info_gain来取info_gain的最大值，并获取对应的分类标签\n",
    "            if info_gain > best_info_gain:  \n",
    "                best_info_gain = info_gain\n",
    "                best_label = label\n",
    "            # 这里保存一下每一次计算的信息增益，便于查看和检查错误\n",
    "            self.gain.setdefault(self.__count, {})  # 建立本次函数调用时的字段，设其value为字典\n",
    "            self.gain[self.__count][label] = info_gain  # 把本次函数调用时计算的各个标签数据存到字典里\n",
    "        self.__count += 1\n",
    "        return best_label  ###返回信息增益最大的对应的a或者b 即以a或者b为根节点\n",
    "    \n",
    "    def _top_amount_level(self, target_list):\n",
    "        class_count = target_list.value_counts().to_dict()  # 计算靶标签的不同水平的样本量，并转化为字典\n",
    "        # 字典的items方法可以将键值对转成[(), (), ...]，可以使用列表方法\n",
    "        #print(class_count)\n",
    "        sorted_class_count = sorted(class_count.items(), key=lambda x:x[1], reverse=True)\n",
    "        #print(sorted_class_count)\n",
    "        return sorted_class_count[0][0]\n",
    "        \n",
    "    def mktree(self, dataSet):\n",
    "        \"\"\"创建决策树\"\"\"\n",
    "        target_list = dataSet.iloc[:, -1]  # target_list 靶标签的那一列数据  即最后一列数据\n",
    "        # 程序终止条件一: 靶标签(数据集的最后一列因变量)在该数据集上只有一个水平，返回该水平\n",
    "        if target_list.unique().shape[0] <= 1:\n",
    "            return target_list[0] # ！！！  即已分类完成\n",
    "        # 程序终止条件二: 数据集只剩下把标签这一列数据；返回数量最多的水平\n",
    "        if dataSet.shape[1] == 1:\n",
    "            return self._top_amount_level(target_list)\n",
    "        # 不满足终止条件时，做如下递归处理\n",
    "        # 1.选择最佳分类标签\n",
    "        best_label = self._best_split(dataSet)   \n",
    "        # 2.递归计算最佳分类标签的不同水平的子数据集的信息增益\n",
    "        #   各个子数据集的最佳分类标签的不同水平...\n",
    "        #   ... \n",
    "        #   直至递归结束\n",
    "        best_label_levels = dataSet[best_label].unique().tolist()\n",
    "        tree = {best_label: {}}    # 生成字典，用于保存树状分类信息；这里不能用self.tree = {}存储\n",
    "        for level in best_label_levels:\n",
    "            level_subdata = self._split_dataSet(dataSet, best_label, level)  # 获取该水平的子数据集\n",
    "            tree[best_label][level] = self.mktree(level_subdata)  # 返回结果\n",
    "        return tree\n",
    "    \n",
    "    def predict(self, tree, labels, test_sample):\n",
    "        \"\"\"\n",
    "        对单个样本进行分类\n",
    "        tree: 训练的字典\n",
    "        labels: 除去最后一列的其它字段\n",
    "        test_sample: 需要分类的一行记录数据\n",
    "        \"\"\"\n",
    "        firstStr = list(tree.keys())[0]           # tree字典里找到第一个用于分类键值对\n",
    "        secondDict = tree[firstStr]\n",
    "        featIndex = labels.index(firstStr)  # 找到第一个建(label)在给定label的索引\n",
    "        for key in secondDict.keys():\n",
    "            if test_sample[featIndex] == key:  # 找到test_sample在当前label下的值\n",
    "                if secondDict[key].__class__.__name__ == \"dict\":\n",
    "                    classLabel = self.predict(secondDict[key], labels, test_sample)\n",
    "                else:\n",
    "                    classLabel = secondDict[key]\n",
    "        return classLabel\n",
    "    \n",
    "    def _unit_test(self):\n",
    "        \"\"\"用于测试_entropy函数\"\"\"\n",
    "        data = [[1, 1, \"yes\"], \n",
    "                [1, 1, \"yes\"],\n",
    "                [1, 0, \"no\"],\n",
    "                [0, 1, \"no\"],\n",
    "                [0, 1, \"no\"],]\n",
    "        data = pd.DataFrame(data=data, columns=[\"a\", \"b\", \"c\"])\n",
    "        #return data # 到此行，用于测试_entropy\n",
    "        #return self._split_dataSet(data, \"a\", 1)  # 到此行，用于测试_split_dataSet\n",
    "        #return self._best_split(data)  # 到此行，用于测试_best_split\n",
    "        #return self.mktree(self.dataSet)  # 到此行，用于测试主程序mktree\n",
    "        self.tree = self.mktree(self.dataSet)  # 到此行，用于测试主程序mktree\n",
    "        labels = [\"age\", \"income\", \"student\", \"reputation\"]\n",
    "        test_sample = [0, 1, 0, 0]   # [0, 1, 0, 0, \"no\"]\n",
    "        outcome = self.predict(self.tree, labels, test_sample)\n",
    "        print(\"The truth class is %s, The ID3Tree outcome is %s.\" % (\"no\", outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The truth class is no, The ID3Tree outcome is no.\n",
      "{\n",
      "  \"0\": {\n",
      "    \"age\": 0.2657121273840979,\n",
      "    \"income\": 0.01774123883005596,\n",
      "    \"student\": 0.1738568696347308,\n",
      "    \"reputation\": 0.04631324460790964\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"income\": 0.4591479170272448,\n",
      "    \"student\": 0.9182958340544896,\n",
      "    \"reputation\": 0.044110417748401076\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"income\": 0.044110417748401076,\n",
      "    \"student\": 0.050484873918346995,\n",
      "    \"reputation\": 0.9182958340544896\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"age\": {\n",
      "    \"0\": {\n",
      "      \"student\": {\n",
      "        \"0\": \"no\",\n",
      "        \"1\": \"yes\"\n",
      "      }\n",
      "    },\n",
      "    \"1\": \"yes\",\n",
      "    \"2\": {\n",
      "      \"reputation\": {\n",
      "        \"0\": \"yes\",\n",
      "        \"1\": \"no\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = ID3Tree()\n",
    "model._unit_test()\n",
    "print(json.dumps(model.gain, indent=\"  \"))  # 可以查看每次递归时的信息熵\n",
    "print(json.dumps(model.tree, indent=\"  \"))  # 查看树\n",
    "\n",
    "# The truth class is no, The ID3Tree outcome is no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0, criterion=\"entropy\", )\n",
    "data = np.array(model.dataSet.iloc[:, :-1])  # model是上面代码的model\n",
    "target = np.array(model.dataSet.iloc[:, -1])\n",
    "clf.fit(data, target)\n",
    "clf.predict([data[0]])  # 预测第一条数据\n",
    "\n",
    "# array(['no'], dtype=object)  # target[0]也为no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./ID3/5.png\" width=640 height=640 />\n",
    "<img src=\"./ID3/6.png\" width=640 height=640 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
